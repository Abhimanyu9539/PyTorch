{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "753619fbf5684f90a165f3cd22e4e358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_749cc15b3464414d9367cbbd6979ebae",
              "IPY_MODEL_e938b94061694ca39fa226e06350f46a",
              "IPY_MODEL_4d01ec063e6a4d54ba887449931eb381"
            ],
            "layout": "IPY_MODEL_f88a615f9ec341a78aff10fb14ef0200"
          }
        },
        "749cc15b3464414d9367cbbd6979ebae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9693e0fd8de84334b6ae4a51d291d103",
            "placeholder": "​",
            "style": "IPY_MODEL_d8d9c1793dcb42789d004221ed569fc2",
            "value": "  0%"
          }
        },
        "e938b94061694ca39fa226e06350f46a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27a908078bfe46ffa232c99cf54eb81b",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cefe729eeaf146639ca2878360caebb3",
            "value": 0
          }
        },
        "4d01ec063e6a4d54ba887449931eb381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2b80be15c884307bde999d6adedf0c2",
            "placeholder": "​",
            "style": "IPY_MODEL_e5ac4c4dad0646e8aabd7be716091332",
            "value": " 0/5 [00:00&lt;?, ?it/s]"
          }
        },
        "f88a615f9ec341a78aff10fb14ef0200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9693e0fd8de84334b6ae4a51d291d103": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8d9c1793dcb42789d004221ed569fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27a908078bfe46ffa232c99cf54eb81b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cefe729eeaf146639ca2878360caebb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2b80be15c884307bde999d6adedf0c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5ac4c4dad0646e8aabd7be716091332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82a5552fb4524cd58a39cf3ec9153333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c93b333449254e55b08a3aef7c4e1ece",
              "IPY_MODEL_a72278b81f98430aaeb8b8e4619e1d07",
              "IPY_MODEL_a77d8ccc7d094453b9222f8115ccd1ff"
            ],
            "layout": "IPY_MODEL_749e4e00e893436db155224e67ecfcf6"
          }
        },
        "c93b333449254e55b08a3aef7c4e1ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0b4a71336e94dadbe5b42bbe9dab4a2",
            "placeholder": "​",
            "style": "IPY_MODEL_f7281f11669044b2a5fdf5ef9640be40",
            "value": "Training Epoch 0:   0%"
          }
        },
        "a72278b81f98430aaeb8b8e4619e1d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edb3d248e8034c099b21de1b49ced05e",
            "max": 1563,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2116d2b66b64decb7fe51dc0b75a8fe",
            "value": 0
          }
        },
        "a77d8ccc7d094453b9222f8115ccd1ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7406058fa90340e9a91c1f9db999ba86",
            "placeholder": "​",
            "style": "IPY_MODEL_5de287e3792d482bb02054a5e385bc83",
            "value": " 0/1563 [00:00&lt;?, ?it/s]"
          }
        },
        "749e4e00e893436db155224e67ecfcf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0b4a71336e94dadbe5b42bbe9dab4a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7281f11669044b2a5fdf5ef9640be40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edb3d248e8034c099b21de1b49ced05e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2116d2b66b64decb7fe51dc0b75a8fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7406058fa90340e9a91c1f9db999ba86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5de287e3792d482bb02054a5e385bc83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcXlnQvk1ZP4",
        "outputId": "0895b1a4-8777-467a-c635-87e30ef630bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Getting setup"
      ],
      "metadata": {
        "id": "J00B5C4pZ4oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check PyTorch version\n",
        "pt_version = torch.__version__\n",
        "print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n",
        "\n",
        "# Install PyTorch 2.0 if necessary\n",
        "if pt_version.split(\".\")[0] == \"1\": # Check if PyTorch version begins with 1\n",
        "    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    print(\"[INFO] PyTorch 2.x installed, if you're on Google Colab, you may need to restart your runtime.\\\n",
        "          Though as of April 2023, Google Colab comes with PyTorch 2.0 pre-installed.\")\n",
        "    import torch\n",
        "    pt_version = torch.__version__\n",
        "    print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n",
        "else:\n",
        "    print(\"[INFO] PyTorch 2.x installed, you'll be able to use the new features.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLuBvDztaqhw",
        "outputId": "59a0c0d4-53fb-4913-9f25-043cd2537bfa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Current PyTorch version: 2.5.1+cu121 (should be 2.x+)\n",
            "[INFO] PyTorch 2.x installed, you'll be able to use the new features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Get GPU info"
      ],
      "metadata": {
        "id": "JBZbHo_hasMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we're using a NVIDIA GPU\n",
        "if torch.cuda.is_available():\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find(\"failed\") >= 0:\n",
        "    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
        "\n",
        "  # Get GPU name\n",
        "  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  gpu_name = gpu_name[1]\n",
        "  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n",
        "  print(f'GPU name: {GPU_NAME}')\n",
        "\n",
        "  # Get GPU capability score\n",
        "  GPU_SCORE = torch.cuda.get_device_capability()\n",
        "  print(f\"GPU capability score: {GPU_SCORE}\")\n",
        "  if GPU_SCORE >= (8, 0):\n",
        "    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
        "  else:\n",
        "    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
        "\n",
        "  # Print GPU info\n",
        "  print(f\"GPU information:\\n{gpu_info}\")\n",
        "\n",
        "else:\n",
        "  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urd3fVXfawgE",
        "outputId": "2df897a5-d472-4b11-f193-ae836c978183"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU name: Tesla_T4\n",
            "GPU capability score: (7, 5)\n",
            "GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\n",
            "GPU information:\n",
            "Sun Dec 15 09:27:03 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8              12W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Globally set devices"
      ],
      "metadata": {
        "id": "_FntiJx6ayua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Set the device with context manager (requires PyTorch 2.x+)\n",
        "with torch.device(device):\n",
        "    # All tensors created in this block will be on device\n",
        "    layer = torch.nn.Linear(20, 30)\n",
        "    print(f\"Layer weights are on device: {layer.weight.device}\")\n",
        "    print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtBQShNqclKl",
        "outputId": "3ea3c010-bab1-4f6c-eee1-72d8e4e0e48e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer weights are on device: cuda:0\n",
            "Layer creating data on device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# set the device\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Set the device globally\n",
        "torch.set_default_device(device)\n",
        "\n",
        "# All tensors created in this block will be on device\n",
        "layer = torch.nn.Linear(20, 30)\n",
        "print(f\"Layer weights are on device: {layer.weight.device}\")\n",
        "print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iWQWMi1csp0",
        "outputId": "e6e24ecb-e59b-4ab1-ab5a-49fb87f82dea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer weights are on device: cuda:0\n",
            "Layer creating data on device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# # Set the device globally\n",
        "# torch.set_default_device(\"cpu\")\n",
        "\n",
        "# # All tensors created will be on \"cpu\"\n",
        "# layer = torch.nn.Linear(20, 30)\n",
        "# print(f\"Layer weights are on device: {layer.weight.device}\")\n",
        "# print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
      ],
      "metadata": {
        "id": "oCf_UoJTdTAj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setting up the experiments"
      ],
      "metadata": {
        "id": "SjgZPXHMdZj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"TorchVision version: {torchvision.__version__}\")\n",
        "\n",
        "# Set the target device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS86EXj1dZgc",
        "outputId": "882197c5-ccad-4dbd-8fef-8dc5101fba51"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "TorchVision version: 0.20.1+cu121\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Create model and transforms"
      ],
      "metadata": {
        "id": "WTwOyXY2dZdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model weights and transforms\n",
        "model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2 # <- use the latest weights (could also use .DEFAULT)\n",
        "transforms = model_weights.transforms()\n",
        "\n",
        "# Setup model\n",
        "model = torchvision.models.resnet50(weights=model_weights)\n",
        "\n",
        "# Count the number of parameters in the model\n",
        "total_params = sum(\n",
        "    param.numel() for param in model.parameters() # <- all params\n",
        "\t# param.numel() for param in model.parameters() if param.requires_grad # <- only trainable params\n",
        ")\n",
        "\n",
        "print(f\"Total parameters of model: {total_params} (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\")\n",
        "print(f\"Model transforms:\\n{transforms}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJsQpV8WdZa6",
        "outputId": "fb134054-a8c2-4eed-aef9-cd7d61c7bc33"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters of model: 25557032 (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\n",
            "Model transforms:\n",
            "ImageClassification(\n",
            "    crop_size=[224]\n",
            "    resize_size=[232]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BILINEAR\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_classes=10):\n",
        "  \"\"\"\n",
        "  Creates a ResNet50 model with the latest weights and transforms via torchvision.\n",
        "  \"\"\"\n",
        "  model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
        "  transforms = model_weights.transforms()\n",
        "  model = torchvision.models.resnet50(weights=model_weights)\n",
        "\n",
        "  # Adjust the number of output features in model to match the number of classes in the dataset\n",
        "  model.fc = torch.nn.Linear(in_features=2048,\n",
        "                             out_features=num_classes)\n",
        "  return model, transforms\n",
        "\n",
        "model, transforms = create_model()"
      ],
      "metadata": {
        "id": "duGrNvEUdZYG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Speedups are most noticeable when a large portion of the GPU is being used"
      ],
      "metadata": {
        "id": "WI7UoGfQdZVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Checking the memory limits of our GPU"
      ],
      "metadata": {
        "id": "5VvLUtKjdZSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check available GPU memory and total GPU memory\n",
        "total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n",
        "print(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\")\n",
        "print(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONu6_GI4dZPs",
        "outputId": "e86864fa-0406-49db-b215-7c15652e3713"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total free GPU memory: 15.453 GB\n",
            "Total GPU memory: 15.836 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size depending on amount of GPU memory\n",
        "total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\n",
        "if total_free_gpu_memory_gb >= 16:\n",
        "  BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.\n",
        "  IMAGE_SIZE = 224\n",
        "  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")\n",
        "else:\n",
        "  BATCH_SIZE = 32\n",
        "  IMAGE_SIZE = 128\n",
        "  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYnEjEqxm782",
        "outputId": "36dea1ae-4299-4299-ef80-1f61ed3beda3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory available is 15.453 GB, using batch size of 32 and image size 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.crop_size = IMAGE_SIZE\n",
        "transforms.resize_size = IMAGE_SIZE\n",
        "print(f\"Updated data transforms:\\n{transforms}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfQ8hXK_m-lX",
        "outputId": "6fa8f772-eba2-4c21-c1d2-df72cf6718ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated data transforms:\n",
            "ImageClassification(\n",
            "    crop_size=128\n",
            "    resize_size=128\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BILINEAR\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 More potential speedups with TF32"
      ],
      "metadata": {
        "id": "uww-Si4LnCYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if GPU_SCORE >= (8, 0):\n",
        "  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, enabling TensorFloat32 (TF32) computing (faster on new GPUs)\")\n",
        "  torch.backends.cuda.matmul.allow_tf32 = True\n",
        "else:\n",
        "  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, TensorFloat32 (TF32) not available, to use it you need a GPU with score >= (8, 0)\")\n",
        "  torch.backends.cuda.matmul.allow_tf32 = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwknpwnfnFs7",
        "outputId": "9e92c1ef-7290-46a8-c4f9-d016572ce939"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU with score: (7, 5), TensorFloat32 (TF32) not available, to use it you need a GPU with score >= (8, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Preparing datasets"
      ],
      "metadata": {
        "id": "YhJe8Sz5nG8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test datasets\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='.',\n",
        "                                             train=True,\n",
        "                                             download=True,\n",
        "                                             transform=transforms)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='.',\n",
        "                                            train=False, # want the test split\n",
        "                                            download=True,\n",
        "                                            transform=transforms)\n",
        "\n",
        "# Get the lengths of the datasets\n",
        "train_len = len(train_dataset)\n",
        "test_len = len(test_dataset)\n",
        "\n",
        "print(f\"[INFO] Train dataset length: {train_len}\")\n",
        "print(f\"[INFO] Test dataset length: {test_len}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXACFPTfnKSh",
        "outputId": "4c321091-6a3d-4297-e9b6-95ee58d80108"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Train dataset length: 50000\n",
            "[INFO] Test dataset length: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Create DataLoaders"
      ],
      "metadata": {
        "id": "6F9FAWi-nLjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Create DataLoaders\n",
        "import os\n",
        "NUM_WORKERS = os.cpu_count() # <- use all available CPU cores (this number can be tweaked through experimentation but generally more workers means faster dataloading from CPU to GPU)\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              sampler=RandomSampler(train_dataset, generator=torch.Generator(device=device)), # Specify generator for RandomSampler\n",
        "                              num_workers=NUM_WORKERS)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              sampler=SequentialSampler(test_dataset),\n",
        "                              num_workers=NUM_WORKERS)\n",
        "\n",
        "# Print details\n",
        "print(f\"Train dataloader length: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\n",
        "print(f\"Test dataloader length: {len(test_dataloader)} batches of size {BATCH_SIZE}\")\n",
        "print(f\"Using number of workers: {NUM_WORKERS} (generally more workers means faster dataloading from CPU to GPU)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQv12BSxnQWE",
        "outputId": "3b66d141-a5e6-40d1-976b-7f90fbabd656"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataloader length: 1563 batches of size 32\n",
            "Test dataloader length: 313 batches of size 32\n",
            "Using number of workers: 2 (generally more workers means faster dataloading from CPU to GPU)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7 Create training and testing loops"
      ],
      "metadata": {
        "id": "hGat9Lh6nRUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def train_step(epoch: int,\n",
        "               model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device,\n",
        "               disable_progress_bar: bool = False) -> Tuple[float, float]:\n",
        "  \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to training mode and then\n",
        "  runs through all of the required training steps (forward\n",
        "  pass, loss calculation, optimizer step).\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "  \"\"\"\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  progress_bar = tqdm(\n",
        "        enumerate(dataloader),\n",
        "        desc=f\"Training Epoch {epoch}\",\n",
        "        total=len(dataloader),\n",
        "        disable=disable_progress_bar\n",
        "    )\n",
        "\n",
        "  for batch, (X, y) in progress_bar:\n",
        "      # Send data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # 2. Calculate  and accumulate loss\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backward\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "\n",
        "      # Calculate and accumulate accuracy metrics across all batches\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "      # Update progress bar\n",
        "      progress_bar.set_postfix(\n",
        "            {\n",
        "                \"train_loss\": train_loss / (batch + 1),\n",
        "                \"train_acc\": train_acc / (batch + 1),\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(epoch: int,\n",
        "              model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device,\n",
        "              disable_progress_bar: bool = False) -> Tuple[float, float]:\n",
        "  \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "  a forward pass on a testing dataset.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "  \"\"\"\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  progress_bar = tqdm(\n",
        "      enumerate(dataloader),\n",
        "      desc=f\"Testing Epoch {epoch}\",\n",
        "      total=len(dataloader),\n",
        "      disable=disable_progress_bar\n",
        "  )\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.no_grad(): # no_grad() required for PyTorch 2.0, I found some errors with `torch.inference_mode()`, please let me know if this is not the case\n",
        "      # Loop through DataLoader batches\n",
        "      for batch, (X, y) in progress_bar:\n",
        "          # Send data to target device\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # 1. Forward pass\n",
        "          test_pred_logits = model(X)\n",
        "\n",
        "          # 2. Calculate and accumulate loss\n",
        "          loss = loss_fn(test_pred_logits, y)\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          # Calculate and accumulate accuracy\n",
        "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "          # Update progress bar\n",
        "          progress_bar.set_postfix(\n",
        "              {\n",
        "                  \"test_loss\": test_loss / (batch + 1),\n",
        "                  \"test_acc\": test_acc / (batch + 1),\n",
        "              }\n",
        "          )\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device,\n",
        "          disable_progress_bar: bool = False) -> Dict[str, List]:\n",
        "  \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "  Passes a target PyTorch models through train_step() and test_step()\n",
        "  functions for a number of epochs, training and testing the model\n",
        "  in the same epoch loop.\n",
        "\n",
        "  Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "                  train_acc: [...],\n",
        "                  test_loss: [...],\n",
        "                  test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "                 {train_loss: [2.0616, 1.0537],\n",
        "                  train_acc: [0.3945, 0.3945],\n",
        "                  test_loss: [1.2641, 1.5706],\n",
        "                  test_acc: [0.3400, 0.2973]}\n",
        "  \"\"\"\n",
        "  # Create empty results dictionary\n",
        "  results = {\"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": [],\n",
        "      \"train_epoch_time\": [],\n",
        "      \"test_epoch_time\": []\n",
        "  }\n",
        "\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs), disable=disable_progress_bar):\n",
        "\n",
        "      # Perform training step and time it\n",
        "      train_epoch_start_time = time.time()\n",
        "      train_loss, train_acc = train_step(epoch=epoch,\n",
        "                                        model=model,\n",
        "                                        dataloader=train_dataloader,\n",
        "                                        loss_fn=loss_fn,\n",
        "                                        optimizer=optimizer,\n",
        "                                        device=device,\n",
        "                                        disable_progress_bar=disable_progress_bar)\n",
        "      train_epoch_end_time = time.time()\n",
        "      train_epoch_time = train_epoch_end_time - train_epoch_start_time\n",
        "\n",
        "      # Perform testing step and time it\n",
        "      test_epoch_start_time = time.time()\n",
        "      test_loss, test_acc = test_step(epoch=epoch,\n",
        "                                      model=model,\n",
        "                                      dataloader=test_dataloader,\n",
        "                                      loss_fn=loss_fn,\n",
        "                                      device=device,\n",
        "                                      disable_progress_bar=disable_progress_bar)\n",
        "      test_epoch_end_time = time.time()\n",
        "      test_epoch_time = test_epoch_end_time - test_epoch_start_time\n",
        "\n",
        "      # Print out what's happening\n",
        "      print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f} | \"\n",
        "          f\"train_epoch_time: {train_epoch_time:.4f} | \"\n",
        "          f\"test_epoch_time: {test_epoch_time:.4f}\"\n",
        "      )\n",
        "\n",
        "      # Update results dictionary\n",
        "      results[\"train_loss\"].append(train_loss)\n",
        "      results[\"train_acc\"].append(train_acc)\n",
        "      results[\"test_loss\"].append(test_loss)\n",
        "      results[\"test_acc\"].append(test_acc)\n",
        "      results[\"train_epoch_time\"].append(train_epoch_time)\n",
        "      results[\"test_epoch_time\"].append(test_epoch_time)\n",
        "\n",
        "  # Return the filled results at the end of the epochs\n",
        "  return results"
      ],
      "metadata": {
        "id": "YH-_KkFenV5n"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Time models across single run"
      ],
      "metadata": {
        "id": "WRrhtacNnW-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.1 Experiment 1 - Single run, no compile"
      ],
      "metadata": {
        "id": "sQqPr1X-neeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of epochs as a constant\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Set the learning rate as a constant (this can be changed to get better results but for now we're just focused on time)\n",
        "LEARNING_RATE = 0.003"
      ],
      "metadata": {
        "id": "8ORb0GqDnebP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-DJNj3Fsn1nX",
        "outputId": "8bae5abb-cc7f-430f-a8e7-af34932b4674"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "model, transforms = create_model()\n",
        "model.to(device)\n",
        "\n",
        "# Create loss function and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "\n",
        "# Train model and track results\n",
        "single_run_no_compile_results = train(model=model,\n",
        "                                      train_dataloader=train_dataloader,\n",
        "                                      test_dataloader=test_dataloader,\n",
        "                                      loss_fn=loss_fn,\n",
        "                                      optimizer=optimizer,\n",
        "                                      epochs=NUM_EPOCHS,\n",
        "                                      device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802,
          "referenced_widgets": [
            "753619fbf5684f90a165f3cd22e4e358",
            "749cc15b3464414d9367cbbd6979ebae",
            "e938b94061694ca39fa226e06350f46a",
            "4d01ec063e6a4d54ba887449931eb381",
            "f88a615f9ec341a78aff10fb14ef0200",
            "9693e0fd8de84334b6ae4a51d291d103",
            "d8d9c1793dcb42789d004221ed569fc2",
            "27a908078bfe46ffa232c99cf54eb81b",
            "cefe729eeaf146639ca2878360caebb3",
            "e2b80be15c884307bde999d6adedf0c2",
            "e5ac4c4dad0646e8aabd7be716091332",
            "82a5552fb4524cd58a39cf3ec9153333",
            "c93b333449254e55b08a3aef7c4e1ece",
            "a72278b81f98430aaeb8b8e4619e1d07",
            "a77d8ccc7d094453b9222f8115ccd1ff",
            "749e4e00e893436db155224e67ecfcf6",
            "e0b4a71336e94dadbe5b42bbe9dab4a2",
            "f7281f11669044b2a5fdf5ef9640be40",
            "edb3d248e8034c099b21de1b49ced05e",
            "f2116d2b66b64decb7fe51dc0b75a8fe",
            "7406058fa90340e9a91c1f9db999ba86",
            "5de287e3792d482bb02054a5e385bc83"
          ]
        },
        "id": "4zvu870vneYy",
        "outputId": "4fc999ae-f00f-41d0-b497-e0cc84b87a15"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "753619fbf5684f90a165f3cd22e4e358"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training Epoch 0:   0%|          | 0/1563 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82a5552fb4524cd58a39cf3ec9153333"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\", line 119, in __getitem__\n    img = self.transform(img)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_presets.py\", line 61, in forward\n    img = F.pil_to_tensor(img)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\", line 209, in pil_to_tensor\n    img = torch.as_tensor(np.array(pic, copy=True))\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 106, in __torch_function__\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2a6a0fd570cc>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Train model and track results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m single_run_no_compile_results = train(model=model,\n\u001b[0m\u001b[1;32m     12\u001b[0m                                       \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                       \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-d639330a841a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device, disable_progress_bar)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0;31m# Perform training step and time it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0mtrain_epoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       train_loss, train_acc = train_step(epoch=epoch, \n\u001b[0m\u001b[1;32m    205\u001b[0m                                         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                                         \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-d639330a841a>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(epoch, model, dataloader, loss_fn, optimizer, device, disable_progress_bar)\u001b[0m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0;31m# Send data to target device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\", line 119, in __getitem__\n    img = self.transform(img)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_presets.py\", line 61, in forward\n    img = F.pil_to_tensor(img)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\", line 209, in pil_to_tensor\n    img = torch.as_tensor(np.array(pic, copy=True))\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 106, in __torch_function__\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Experiment 2 - Single run, with compile"
      ],
      "metadata": {
        "id": "Aj8F-eL0neWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model and transforms\n",
        "model, transforms = create_model()\n",
        "model.to(device)\n",
        "\n",
        "# Create loss function and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "\n",
        "# Compile the model and time how long it takes\n",
        "compile_start_time = time.time()\n",
        "\n",
        "### New in PyTorch 2.x ###\n",
        "compiled_model = torch.compile(model)\n",
        "##########################\n",
        "\n",
        "compile_end_time = time.time()\n",
        "compile_time = compile_end_time - compile_start_time\n",
        "print(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")\n",
        "\n",
        "# Train the compiled model\n",
        "single_run_compile_results = train(model=compiled_model,\n",
        "                                   train_dataloader=train_dataloader,\n",
        "                                   test_dataloader=test_dataloader,\n",
        "                                   loss_fn=loss_fn,\n",
        "                                   optimizer=optimizer,\n",
        "                                   epochs=NUM_EPOCHS,\n",
        "                                   device=device)"
      ],
      "metadata": {
        "id": "wyi4ZEFOneUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Compare the results of experiment 1 and 2"
      ],
      "metadata": {
        "id": "nGk7Xto5neRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn experiment results into dataframes\n",
        "import pandas as pd\n",
        "single_run_no_compile_results_df = pd.DataFrame(single_run_no_compile_results)\n",
        "single_run_compile_results_df = pd.DataFrame(single_run_compile_results)"
      ],
      "metadata": {
        "id": "g2RXgJa6nePX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create filename to save the results\n",
        "DATASET_NAME = \"CIFAR10\"\n",
        "MODEL_NAME = \"ResNet50\""
      ],
      "metadata": {
        "id": "PeoLRc7Endpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_mean_epoch_times(non_compiled_results: pd.DataFrame,\n",
        "                          compiled_results: pd.DataFrame,\n",
        "                          multi_runs: bool=False,\n",
        "                          num_runs: int=0,\n",
        "                          save: bool=False,\n",
        "                          save_path: str=\"\",\n",
        "                          dataset_name: str=DATASET_NAME,\n",
        "                          model_name: str=MODEL_NAME,\n",
        "                          num_epochs: int=NUM_EPOCHS,\n",
        "                          image_size: int=IMAGE_SIZE,\n",
        "                          batch_size: int=BATCH_SIZE) -> plt.figure:\n",
        "\n",
        "    # Get the mean epoch times from the non-compiled models\n",
        "    mean_train_epoch_time = non_compiled_results.train_epoch_time.mean()\n",
        "    mean_test_epoch_time = non_compiled_results.test_epoch_time.mean()\n",
        "    mean_results = [mean_train_epoch_time, mean_test_epoch_time]\n",
        "\n",
        "    # Get the mean epoch times from the compiled models\n",
        "    mean_compile_train_epoch_time = compiled_results.train_epoch_time.mean()\n",
        "    mean_compile_test_epoch_time = compiled_results.test_epoch_time.mean()\n",
        "    mean_compile_results = [mean_compile_train_epoch_time, mean_compile_test_epoch_time]\n",
        "\n",
        "    # Calculate the percentage difference between the mean compile and non-compile train epoch times\n",
        "    train_epoch_time_diff = mean_compile_train_epoch_time - mean_train_epoch_time\n",
        "    train_epoch_time_diff_percent = (train_epoch_time_diff / mean_train_epoch_time) * 100\n",
        "\n",
        "    # Calculate the percentage difference between the mean compile and non-compile test epoch times\n",
        "    test_epoch_time_diff = mean_compile_test_epoch_time - mean_test_epoch_time\n",
        "    test_epoch_time_diff_percent = (test_epoch_time_diff / mean_test_epoch_time) * 100\n",
        "\n",
        "    # Print the mean difference percentages\n",
        "    print(f\"Mean train epoch time difference: {round(train_epoch_time_diff_percent, 3)}% (negative means faster)\")\n",
        "    print(f\"Mean test epoch time difference: {round(test_epoch_time_diff_percent, 3)}% (negative means faster)\")\n",
        "\n",
        "    # Create a bar plot of the mean train and test epoch time for both compiled and non-compiled models\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    width = 0.3\n",
        "    x_indicies = np.arange(len(mean_results))\n",
        "\n",
        "    plt.bar(x=x_indicies, height=mean_results, width=width, label=\"non_compiled_results\")\n",
        "    plt.bar(x=x_indicies + width, height=mean_compile_results, width=width, label=\"compiled_results\")\n",
        "    plt.xticks(x_indicies + width / 2, (\"Train Epoch\", \"Test Epoch\"))\n",
        "    plt.ylabel(\"Mean epoch time (seconds, lower is better)\")\n",
        "\n",
        "    # Create the title based on the parameters passed to the function\n",
        "    if multi_runs:\n",
        "        plt.suptitle(\"Multiple run results\")\n",
        "        plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} ({num_runs} runs) | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")\n",
        "    else:\n",
        "        plt.suptitle(\"Single run results\")\n",
        "        plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")\n",
        "    plt.legend();\n",
        "\n",
        "    # Save the figure\n",
        "    if save:\n",
        "        assert save_path != \"\", \"Please specify a save path to save the model figure to via the save_path parameter.\"\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"[INFO] Plot saved to {save_path}\")"
      ],
      "metadata": {
        "id": "MnxaNM8-ppQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory for saving figures\n",
        "import os\n",
        "dir_to_save_figures_in = \"pytorch_2_results/figures/\"\n",
        "os.makedirs(dir_to_save_figures_in, exist_ok=True)\n",
        "\n",
        "# Create a save path for the single run results\n",
        "save_path_multi_run = f\"{dir_to_save_figures_in}single_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\"\n",
        "print(f\"[INFO] Save path for single run results: {save_path_multi_run}\")\n",
        "\n",
        "# Plot the results and save the figures\n",
        "plot_mean_epoch_times(non_compiled_results=single_run_no_compile_results_df,\n",
        "                      compiled_results=single_run_compile_results_df,\n",
        "                      multi_runs=False,\n",
        "                      save_path=save_path_multi_run,\n",
        "                      save=True)"
      ],
      "metadata": {
        "id": "z2nVhuweppNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Save single run results to file with GPU details"
      ],
      "metadata": {
        "id": "9ZrVWLJKppK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a directory for single_run results\n",
        "import os\n",
        "pytorch_2_results_dir = \"pytorch_2_results\"\n",
        "pytorch_2_single_run_results_dir = f\"{pytorch_2_results_dir}/single_run_results\"\n",
        "os.makedirs(pytorch_2_single_run_results_dir, exist_ok=True)\n",
        "\n",
        "# Create filenames for each of the dataframes\n",
        "save_name_for_non_compiled_results = f\"single_run_non_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n",
        "save_name_for_compiled_results = f\"single_run_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n",
        "\n",
        "# Create filepaths to save the results to\n",
        "single_run_no_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_non_compiled_results}\"\n",
        "single_run_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_compiled_results}\"\n",
        "print(f\"[INFO] Saving non-compiled experiment 1 results to: {single_run_no_compile_save_path}\")\n",
        "print(f\"[INFO] Saving compiled experiment 2 results to: {single_run_compile_save_path}\")\n",
        "\n",
        "# Save the results\n",
        "single_run_no_compile_results_df.to_csv(single_run_no_compile_save_path)\n",
        "single_run_compile_results_df.to_csv(single_run_compile_save_path)"
      ],
      "metadata": {
        "id": "ssjghkPhppIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Time models across multiple runs"
      ],
      "metadata": {
        "id": "Svslr-5xppGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_train_non_compiled_model(epochs=NUM_EPOCHS,\n",
        "                                        learning_rate=LEARNING_RATE,\n",
        "                                        disable_progress_bar=False):\n",
        "    \"\"\"\n",
        "    Create and train a non-compiled PyTorch model.\n",
        "    \"\"\"\n",
        "    model, _ = create_model()\n",
        "    model.to(device)\n",
        "\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=learning_rate)\n",
        "\n",
        "    results = train(model=model,\n",
        "                    train_dataloader=train_dataloader,\n",
        "                    test_dataloader=test_dataloader,\n",
        "                    loss_fn=loss_fn,\n",
        "                    optimizer=optimizer,\n",
        "                    epochs=epochs,\n",
        "                    device=device,\n",
        "                    disable_progress_bar=disable_progress_bar)\n",
        "    return results\n",
        "\n",
        "def create_compiled_model():\n",
        "    \"\"\"\n",
        "    Create a compiled PyTorch model and return it.\n",
        "    \"\"\"\n",
        "    model, _ = create_model()\n",
        "    model.to(device)\n",
        "\n",
        "    compile_start_time = time.time()\n",
        "    ### New in PyTorch 2.x ###\n",
        "    compiled_model = torch.compile(model)\n",
        "    ##########################\n",
        "    compile_end_time = time.time()\n",
        "\n",
        "    compile_time = compile_end_time - compile_start_time\n",
        "\n",
        "    print(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")\n",
        "    return compiled_model\n",
        "\n",
        "def train_compiled_model(model=compiled_model,\n",
        "                         epochs=NUM_EPOCHS,\n",
        "                         learning_rate=LEARNING_RATE,\n",
        "                         disable_progress_bar=False):\n",
        "    \"\"\"\n",
        "    Train a compiled model and return the results.\n",
        "    \"\"\"\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(compiled_model.parameters(),\n",
        "                                 lr=learning_rate)\n",
        "\n",
        "    compile_results = train(model=model,\n",
        "                            train_dataloader=train_dataloader,\n",
        "                            test_dataloader=test_dataloader,\n",
        "                            loss_fn=loss_fn,\n",
        "                            optimizer=optimizer,\n",
        "                            epochs=epochs,\n",
        "                            device=device,\n",
        "                            disable_progress_bar=disable_progress_bar)\n",
        "\n",
        "    return compile_results"
      ],
      "metadata": {
        "id": "FWuYpT9CpyRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Experiment 3 - Multiple runs, no compile"
      ],
      "metadata": {
        "id": "Z0V_-VaQp4Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run non-compiled model for multiple runs\n",
        "NUM_RUNS = 3\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Create an empty list to store multiple run results\n",
        "non_compile_results_multiple_runs = []\n",
        "\n",
        "# Run non-compiled model for multiple runs\n",
        "for i in tqdm(range(NUM_RUNS)):\n",
        "    print(f\"[INFO] Run {i+1} of {NUM_RUNS} for non-compiled model\")\n",
        "    results = create_and_train_non_compiled_model(epochs=NUM_EPOCHS, disable_progress_bar=True)\n",
        "    non_compile_results_multiple_runs.append(results)"
      ],
      "metadata": {
        "id": "5xsbyi9Yp5Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go through non_compile_results_multiple_runs and create a dataframe for each run then concatenate them together\n",
        "non_compile_results_dfs = []\n",
        "for result in non_compile_results_multiple_runs:\n",
        "    result_df = pd.DataFrame(result)\n",
        "    non_compile_results_dfs.append(result_df)\n",
        "non_compile_results_multiple_runs_df = pd.concat(non_compile_results_dfs)\n",
        "\n",
        "# Get the averages across the multiple runs\n",
        "non_compile_results_multiple_runs_df = non_compile_results_multiple_runs_df.groupby(non_compile_results_multiple_runs_df.index).mean()\n",
        "non_compile_results_multiple_runs_df"
      ],
      "metadata": {
        "id": "5YLDtZvup6qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Experiment 4 - Multiple runs, with compile"
      ],
      "metadata": {
        "id": "F2piwvTZp6m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create compiled model\n",
        "compiled_model = create_compiled_model()\n",
        "\n",
        "# Create an empty list to store compiled model results\n",
        "compiled_results_multiple_runs = []\n",
        "\n",
        "# Run compiled model for multiple runs\n",
        "for i in tqdm(range(NUM_RUNS)):\n",
        "    print(f\"[INFO] Run {i+1} of {NUM_RUNS} for compiled model\")\n",
        "    # Train the compiled model (note: the model will only be compiled once and then re-used for subsequent runs)\n",
        "    results = train_compiled_model(model=compiled_model, epochs=NUM_EPOCHS, disable_progress_bar=True)\n",
        "    compiled_results_multiple_runs.append(results)"
      ],
      "metadata": {
        "id": "BRwc5VwVp6kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go through compile_results_multiple_runs and create a dataframe for each run then concatenate them together\n",
        "compile_results_dfs = []\n",
        "for result in compiled_results_multiple_runs:\n",
        "    result_df = pd.DataFrame(result)\n",
        "    compile_results_dfs.append(result_df)\n",
        "compile_results_multiple_runs_df = pd.concat(compile_results_dfs)\n",
        "\n",
        "# Get the averages across the multiple runs\n",
        "compile_results_multiple_runs_df = compile_results_multiple_runs_df.groupby(compile_results_multiple_runs_df.index).mean() # .index = groupby the epoch number\n",
        "compile_results_multiple_runs_df"
      ],
      "metadata": {
        "id": "o8aiAnkAp6iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Compare results of experiment 3 and 4"
      ],
      "metadata": {
        "id": "WJk9DUzip6f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory to save the multi-run figure to\n",
        "os.makedirs(\"pytorch_2_results/figures\", exist_ok=True)\n",
        "\n",
        "# Create a path to save the figure for multiple runs\n",
        "save_path_multi_run = f\"pytorch_2_results/figures/multi_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\"\n",
        "\n",
        "# Plot the mean epoch times for experiment 3 and 4\n",
        "plot_mean_epoch_times(non_compiled_results=non_compile_results_multiple_runs_df,\n",
        "                      compiled_results=compile_results_multiple_runs_df,\n",
        "                      multi_runs=True,\n",
        "                      num_runs=NUM_RUNS,\n",
        "                      save_path=save_path_multi_run,\n",
        "                      save=True)"
      ],
      "metadata": {
        "id": "37lFmGksp6dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Save multi run results to file with GPU details"
      ],
      "metadata": {
        "id": "fy6Z9PtGp6a_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a directory for multi_run results\n",
        "import os\n",
        "pytorch_2_results_dir = \"pytorch_2_results\"\n",
        "pytorch_2_multi_run_results_dir = f\"{pytorch_2_results_dir}/multi_run_results\"\n",
        "os.makedirs(pytorch_2_multi_run_results_dir, exist_ok=True)\n",
        "\n",
        "# Create filenames for each of the dataframes\n",
        "save_name_for_multi_run_non_compiled_results = f\"multi_run_non_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n",
        "save_name_for_multi_run_compiled_results = f\"multi_run_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n",
        "\n",
        "# Create filepaths to save the results to\n",
        "multi_run_no_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_non_compiled_results}\"\n",
        "multi_run_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_compiled_results}\"\n",
        "print(f\"[INFO] Saving experiment 3 non-compiled results to: {multi_run_no_compile_save_path}\")\n",
        "print(f\"[INFO] Saving experiment 4 compiled results to: {multi_run_compile_save_path}\")\n",
        "\n",
        "# Save the results\n",
        "non_compile_results_multiple_runs_df.to_csv(multi_run_no_compile_save_path)\n",
        "compile_results_multiple_runs_df.to_csv(multi_run_compile_save_path)"
      ],
      "metadata": {
        "id": "XjgE0Vc8qGri"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}